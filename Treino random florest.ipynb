{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450233de-afd9-4f21-9473-09b984944440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pylab as plt \n",
    "\n",
    "#dataframe\n",
    "df = pd.read_csv(\"treino_sinais_vitais_com_label1.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce540e-a3ef-4cc3-88a5-b57d90464349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID3\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, precision_score, recall_score, f1_score\n",
    "\n",
    "# 1. Carregar os Dados\n",
    "data = df\n",
    "\n",
    "# 2. Pré-processar os Dados\n",
    "#X = data.iloc[:, 1:-2].values\n",
    "#y = data.iloc[:, -1].values\n",
    "X = data.iloc[:, 1:-2]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# 3. Split the Data \n",
    "#test size eh o quanto dos dados eu vou dar, 0.3 equivale a 70% dos dados, quanto menor, mais preciso as previsões\n",
    "#Eu nunca vou dar todos os dados pra o modelo treinar, eu vou reservar 30% dos dados que o modelo não viu pra testar \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17) \n",
    "\n",
    "# 4. Criar e Treinar o Modelo\n",
    "model = DecisionTreeClassifier(criterion='entropy', random_state=42, ccp_alpha=0.01)\n",
    "#podemos usar o ccp se tiver um overfitting\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Fazer Previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 5. Avaliar o Modelo\n",
    "\n",
    "#Matriz de confusão \n",
    "from sklearn.metrics import confusion_matrix\n",
    "confmatrix = confusion_matrix(y_test, y_pred)\n",
    "print('matrix de confusão')\n",
    "print(confmatrix)\n",
    "\n",
    "# Avaliar a precisão do modelo \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "\n",
    "# Erro Quadrático Médio (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred)) \n",
    "\n",
    "# Precisão (Precision) \n",
    "# true positives / (true positives + false positives) \n",
    "precision = precision_score(y_test, y_pred, average='weighted') # Use 'weighted' para multiclasses \n",
    "\n",
    "# Recall \n",
    "recall = recall_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "# F-measure \n",
    "f_measure = f1_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "# Exibir as Métricas \n",
    "print(f'Precisão (Accuracy): {accuracy:.2f}') \n",
    "print(f'Erro Quadrático Médio (RMSE): {rmse:.2f}') \n",
    "print(f'Precisão (Precision): {precision:.2f}') \n",
    "print(f'Recall: {recall:.2f}') \n",
    "print(f'F-measure: {f_measure:.2f}')\n",
    "\n",
    "\n",
    "# Relatório de Classificação \n",
    "report = classification_report(y_test, y_pred) \n",
    "print('Relatório de Classificação:')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d35b4-341b-464c-a765-1c91d3f626c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importancia de cada dado\n",
    "#model.feature_importances_\n",
    "\n",
    "#O que tiver maior valor foi o que teve mais importancia \n",
    "features = pd.DataFrame(model.feature_importances_, index = X.columns)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36438f59-ebce-4ecd-a88d-770ff9852638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprimindo novamente pra nós vermos o formato dos dados novamente, que utilizaremos no random florest\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a854732-c0ac-46f5-833a-d95848ba2072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Florest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, mean_squared_error\n",
    "\n",
    "data = df\n",
    "\n",
    "#pegando a sgunda coluna ate a penultima\n",
    "X = data.iloc[:, 1:-2]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# 3. Split the Data \n",
    "#test size eh o quanto dos dados eu vou dar, 0.3 equivale a 70% dos dados, quanto menor, mais preciso as previsões\n",
    "#Eu nunca vou dar todos os dados pra o modelo treinar, eu vou reservar 30% dos dados que o modelo não viu pra testar \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=17) \n",
    "\n",
    "# 4. Create and Train the Model \n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# Fazer previsões \n",
    "y_pred = model.predict(X_test) \n",
    "\n",
    "# Avaliar a precisão do modelo \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "\n",
    "# Erro Quadrático Médio (RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred)) \n",
    "\n",
    "# Precisão (Precision) \n",
    "precision = precision_score(y_test, y_pred, average='weighted') # Use 'weighted' para multiclasses \n",
    "\n",
    "# Recall \n",
    "recall = recall_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "# F-measure \n",
    "f_measure = f1_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "# Exibir as Métricas \n",
    "print(f'Precisão (Accuracy): {accuracy:.2f}') \n",
    "print(f'Erro Quadrático Médio (RMSE): {rmse:.2f}') \n",
    "print(f'Precisão (Precision): {precision:.2f}') \n",
    "print(f'Recall: {recall:.2f}') \n",
    "print(f'F-measure: {f_measure:.2f}')\n",
    "\n",
    "\n",
    "# Relatório de Classificação \n",
    "report = classification_report(y_test, y_pred) \n",
    "print('Relatório de Classificação:')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d5558-4096-4d07-9700-2e6b7e9d135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importancia de cada dado\n",
    "#model.feature_importances_\n",
    "\n",
    "#O que tiver maior valor foi o que teve mais importancia \n",
    "features = pd.DataFrame(model.feature_importances_, index = X.columns)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64df36b-cc9f-41b5-92cf-649fbea296ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
